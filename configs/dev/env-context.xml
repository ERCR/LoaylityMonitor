<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:hdp="http://www.springframework.org/schema/hadoop"
       xmlns:batch="http://www.springframework.org/schema/batch"
       xmlns:context="http://www.springframework.org/schema/context"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd
       http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd
       http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-2.1.xsd
       http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd">

    <hdp:pig-factory exec-type="MAPREDUCE" configuration-ref="hadoopConfiguration" />

    <!-- groovy script that push data to timed directory (preparation step)
     Scope 'step' is obligated to provide access to jobParameters
     -->
    <hdp:script-tasklet id="pickup-processing-directory" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            // fs - org.apache.hadoop.fs.FileSystem
            // fsh - org.springframework.data.hadoop.fs.FsShell instance based on 'hadoopConfiguration' bean
            // distcp	org.springframework.data.hadoop.fs.DistributedCopyUtil	Programmatic access to DistCp
            // jobParamaters = job parameters from spring batch

            if( !fsh.rename(${etl.logs.input}, ${etl.logs.working}) ) throw new RuntimeException("${etl.logs.input} can't be renamed to ${etl.logs.working}")

        </hdp:script>
    </hdp:script-tasklet>

    <hdp:script-tasklet id="base-agg-processing" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            // fs - org.apache.hadoop.fs.FileSystem
            // fsh - org.springframework.data.hadoop.fs.FsShell instance based on 'hadoopConfiguration' bean
            // distcp	org.springframework.data.hadoop.fs.DistributedCopyUtil	Programmatic access to DistCp
            // jobParamaters = job parameters from spring batch

            println "Perform base aggregation procesing"

        </hdp:script>
    </hdp:script-tasklet>


    <hdp:script-tasklet id="import-into-db" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            // fs - org.apache.hadoop.fs.FileSystem
            // fsh - org.springframework.data.hadoop.fs.FsShell instance based on 'hadoopConfiguration' bean
            // distcp	org.springframework.data.hadoop.fs.DistributedCopyUtil	Programmatic access to DistCp
            // jobParamaters = job parameters from spring batch

            println "Perform data import into DB"

        </hdp:script>
    </hdp:script-tasklet>


    <hdp:script-tasklet id="delete-processed-dirs" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            // fs - org.apache.hadoop.fs.FileSystem
            // fsh - org.springframework.data.hadoop.fs.FsShell instance based on 'hadoopConfiguration' bean
            // distcp	org.springframework.data.hadoop.fs.DistributedCopyUtil	Programmatic access to DistCp
            // jobParamaters = job parameters from spring batch

            println "delete processed dirs and clean up lock"

        </hdp:script>
    </hdp:script-tasklet>

</beans>